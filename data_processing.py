#ANUJ - renamed your file for clarity

# -*- coding: utf-8 -*-
"""DataScience_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zhC0F6B6dO7hts70nIqj0ocgephK1ZLu
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def loadFile(path):
  names = ["id","title","author","text","label"]
  df = pd.read_csv(path,sep = ",",names= names,header = 0)
  df.dropna(how='any', inplace=True)
  return df

def cleanFile(data):

  
  #Drop NA
  data.dropna(how='any')

  #Remove stop words, sppecial characters and numbers from text
  #nltk.download('stopwords')
  #nltk.download('punkt')
  stop_words = set(stopwords.words('english'))  
  filtered_sentence = []
  filtered_title = []
  for t in data.text: 
    #remove special characters and numbers
    t = re.sub(r"\W+|_", " ", t)
    t = re.sub(r'[0-9]+', '', t)
    #Convert to lower case and tokenize
    word_tokens = word_tokenize(t.lower())
    #remove stop words
    filtered_sentence.append(' '.join([w for w in word_tokens if not w in stop_words]))
  for t in data.title: 
    #remove special characters and numbers
    t = re.sub(r"\W+|_", " ", t)
    t = re.sub(r'[0-9]+', '', t)
    #Convert to lower case and tokenize
    word_tokens = word_tokenize(t.lower())
    #remove stop words
    filtered_title.append(' '.join([w for w in word_tokens if not w in stop_words]))

  data['filteredText'] = filtered_sentence
  data['filteredTitle'] = filtered_title

  data['filteredText'] = filtered_sentence

  return data

def main():
  df = loadFile('') #Enter file Path
  df_clean = cleanFile(df)
  df_clean.head()

  if __name__ == "__main__":
    #main()
    df = loadFile('') #Enter file path
    df_clean = cleanFile(df)

  print(df_clean.head())
